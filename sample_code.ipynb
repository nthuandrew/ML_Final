{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrewchen/ML_Final/ML_final_py/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 384)\n",
      "    (token_type_embeddings): Embedding(2, 384)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
      "        [ 101, 2169, 6251, 2003, 4991,  102,    0]])\n",
      "Sentence embeddings:\n",
      "tensor([[ 6.7657e-02,  6.3496e-02,  4.8713e-02,  7.9305e-02,  3.7448e-02,\n",
      "          2.6528e-03,  3.9375e-02, -7.0985e-03,  5.9361e-02,  3.1537e-02,\n",
      "          6.0098e-02, -5.2905e-02,  4.0607e-02, -2.5931e-02,  2.9843e-02,\n",
      "          1.1269e-03,  7.3515e-02, -5.0382e-02, -1.2239e-01,  2.3703e-02,\n",
      "          2.9727e-02,  4.2477e-02,  2.5634e-02,  1.9952e-03, -5.6919e-02,\n",
      "         -2.7160e-02, -3.2904e-02,  6.6025e-02,  1.1901e-01, -4.5879e-02,\n",
      "         -7.2621e-02, -3.2584e-02,  5.2341e-02,  4.5055e-02,  8.2531e-03,\n",
      "          3.6702e-02, -1.3942e-02,  6.5392e-02, -2.6427e-02,  2.0641e-04,\n",
      "         -1.3664e-02, -3.6281e-02, -1.9504e-02, -2.8974e-02,  3.9427e-02,\n",
      "         -8.8409e-02,  2.6243e-03,  1.3671e-02,  4.8306e-02, -3.1157e-02,\n",
      "         -1.1733e-01, -5.1169e-02, -8.8529e-02, -2.1896e-02,  1.4299e-02,\n",
      "          4.4417e-02, -1.3482e-02,  7.4339e-02,  2.6638e-02, -1.9876e-02,\n",
      "          1.7919e-02, -1.0605e-02, -9.0426e-02,  2.1327e-02,  1.4120e-01,\n",
      "         -6.4717e-03, -1.4038e-03, -1.5361e-02, -8.7357e-02,  7.2217e-02,\n",
      "          2.0140e-02,  4.2559e-02, -3.4901e-02,  3.1951e-04, -8.0297e-02,\n",
      "         -3.2747e-02,  2.8527e-02, -5.1366e-02,  1.0939e-01,  8.1933e-02,\n",
      "         -9.8404e-02, -9.3410e-02, -1.5129e-02,  4.5125e-02,  4.9417e-02,\n",
      "         -2.5187e-02,  1.5708e-02, -1.2929e-01,  5.3189e-03,  4.0234e-03,\n",
      "         -2.3457e-02, -6.7298e-02,  2.9228e-02, -2.6085e-02,  1.3063e-02,\n",
      "         -3.1166e-02, -4.8271e-02, -5.5886e-02, -3.8750e-02,  1.2001e-01,\n",
      "         -1.0392e-02,  4.8970e-02,  5.5354e-02,  4.4936e-02, -4.0098e-03,\n",
      "         -1.0296e-01, -2.9297e-02, -5.8340e-02,  2.7047e-02, -2.2017e-02,\n",
      "         -7.2224e-02, -4.1387e-02, -1.9330e-02,  2.7333e-03,  2.7700e-04,\n",
      "         -9.6759e-02, -1.0057e-01, -1.4192e-02, -8.0789e-02,  4.5392e-02,\n",
      "          2.4504e-02,  5.9761e-02, -7.3818e-02,  1.1984e-02, -6.6340e-02,\n",
      "         -7.6905e-02,  3.8516e-02, -5.5936e-33,  2.8001e-02, -5.6078e-02,\n",
      "         -4.8660e-02,  2.1557e-02,  6.0198e-02, -4.8140e-02, -3.5025e-02,\n",
      "          1.9331e-02, -1.7515e-02, -3.8921e-02, -3.8107e-03, -1.7029e-02,\n",
      "          2.8210e-02,  1.2829e-02,  4.7160e-02,  6.2103e-02, -6.4359e-02,\n",
      "          1.2929e-01, -1.3123e-02,  5.2307e-02, -3.7368e-02,  2.8909e-02,\n",
      "         -1.6898e-02, -2.3733e-02, -3.3349e-02, -5.1676e-02,  1.5536e-02,\n",
      "          2.0880e-02, -1.2537e-02,  4.5958e-02,  3.7272e-02,  2.8057e-02,\n",
      "         -5.9001e-02, -1.1699e-02,  4.9218e-02,  4.7033e-02,  7.3549e-02,\n",
      "         -3.7053e-02,  3.9846e-03,  1.0641e-02, -1.6148e-04, -5.2717e-02,\n",
      "          2.7593e-02, -3.9292e-02,  8.4472e-02,  4.8686e-02, -4.8587e-03,\n",
      "          1.7995e-02, -4.2857e-02,  1.2338e-02,  6.3995e-03,  4.0482e-02,\n",
      "          1.4889e-02, -1.5394e-02,  7.6295e-02,  2.3704e-02,  4.4524e-02,\n",
      "          5.0820e-02, -2.3125e-03, -1.8874e-02, -1.2334e-02,  4.6600e-02,\n",
      "         -5.6344e-02,  6.2993e-02, -3.1554e-02,  3.2491e-02,  2.3467e-02,\n",
      "         -6.5544e-02,  2.0171e-02,  2.5708e-02, -1.2387e-02, -8.3649e-03,\n",
      "         -6.6438e-02,  9.4307e-02, -3.5709e-02, -3.4248e-02, -6.6636e-03,\n",
      "         -8.0153e-03, -3.0971e-02,  4.3301e-02, -8.2140e-03, -1.5079e-01,\n",
      "          3.0769e-02,  4.0072e-02, -3.7929e-02,  1.9322e-03,  4.0053e-02,\n",
      "         -8.7708e-02, -3.6849e-02,  8.5796e-03, -3.1925e-02, -1.2526e-02,\n",
      "          7.3554e-02,  1.3473e-03,  2.0592e-02,  2.7110e-33, -5.1858e-02,\n",
      "          5.7836e-02, -9.1898e-02,  3.9442e-02,  1.0558e-01, -1.9691e-02,\n",
      "          6.1840e-02, -7.6347e-02,  2.4088e-02,  9.4005e-02, -1.1654e-01,\n",
      "          3.7120e-02,  5.2243e-02, -3.9586e-03,  5.7221e-02,  5.3285e-03,\n",
      "          1.2402e-01,  1.3902e-02, -1.1025e-02,  3.5605e-02, -3.3075e-02,\n",
      "          8.1657e-02, -1.5200e-02,  6.0559e-02, -6.0140e-02,  3.2610e-02,\n",
      "         -3.4830e-02, -1.6988e-02, -9.7491e-02, -2.7148e-02,  1.7471e-03,\n",
      "         -7.6898e-02, -4.3186e-02, -1.8998e-02, -2.9166e-02,  5.7749e-02,\n",
      "          2.4182e-02, -1.1690e-02, -6.2144e-02,  2.8435e-02, -2.3752e-04,\n",
      "         -2.5178e-02,  4.3964e-03,  8.1284e-02,  3.6418e-02, -6.0401e-02,\n",
      "         -3.6552e-02, -7.9375e-02, -5.0853e-03,  6.6970e-02, -1.1778e-01,\n",
      "          3.2374e-02, -4.7125e-02, -1.3446e-02, -9.4844e-02,  8.2496e-03,\n",
      "         -1.0675e-02, -6.8188e-02,  1.1182e-03,  2.4802e-02, -6.3589e-02,\n",
      "          2.8449e-02, -2.6130e-02,  8.5811e-02,  1.1468e-01, -5.3535e-02,\n",
      "         -5.6359e-02,  4.2601e-02,  1.0945e-02,  2.0958e-02,  1.0013e-01,\n",
      "          3.2605e-02, -1.8421e-01, -3.9321e-02, -6.9145e-02, -6.3810e-02,\n",
      "         -6.5639e-02, -6.4125e-03, -4.7961e-02, -7.6813e-02,  2.9538e-02,\n",
      "         -2.2995e-02,  4.1704e-02, -2.5005e-02, -4.5450e-03, -4.1714e-02,\n",
      "         -1.3229e-02, -6.3836e-02, -2.4647e-03, -1.3734e-02,  1.6898e-02,\n",
      "         -6.3040e-02,  8.9888e-02,  4.1817e-02, -1.8569e-02, -1.8044e-08,\n",
      "         -1.6800e-02, -3.2158e-02,  6.3038e-02, -4.1309e-02,  4.4482e-02,\n",
      "          2.0246e-03,  6.2959e-02, -5.1737e-03, -1.0044e-02, -3.0564e-02,\n",
      "          3.5267e-02,  5.5858e-02, -4.6712e-02,  3.4510e-02,  3.2958e-02,\n",
      "          4.3011e-02,  2.9436e-02, -3.0316e-02, -1.7111e-02,  7.3748e-02,\n",
      "         -5.4791e-02,  2.7752e-02,  6.2017e-03,  1.5880e-02,  3.4298e-02,\n",
      "         -5.1575e-03,  2.3508e-02,  7.5314e-02,  1.9284e-02,  3.3620e-02,\n",
      "          5.0910e-02,  1.5250e-01,  1.6421e-02,  2.7053e-02,  3.7516e-02,\n",
      "          2.1855e-02,  5.6633e-02, -3.9575e-02,  7.1231e-02, -5.4138e-02,\n",
      "          1.0376e-03,  2.1185e-02, -3.5631e-02,  1.0902e-01,  2.7653e-03,\n",
      "          3.1400e-02,  1.3842e-03, -3.4574e-02, -4.5928e-02,  2.8808e-02,\n",
      "          7.1690e-03,  4.8469e-02,  2.6102e-02, -9.4407e-03,  2.8217e-02,\n",
      "          3.4872e-02,  3.6910e-02, -8.5895e-03, -3.5321e-02, -2.4786e-02,\n",
      "         -1.9192e-02,  3.8071e-02,  5.9965e-02, -4.2229e-02],\n",
      "        [ 8.6439e-02,  1.0276e-01,  5.3946e-03,  2.0445e-03, -9.9633e-03,\n",
      "          2.5385e-02,  4.9288e-02, -3.0627e-02,  6.8725e-02,  1.0137e-02,\n",
      "          7.7540e-02, -9.0081e-02,  6.1062e-03, -5.6990e-02,  1.4171e-02,\n",
      "          2.8049e-02, -8.6846e-02,  7.6440e-02, -1.0349e-01, -6.7744e-02,\n",
      "          6.9995e-02,  8.4425e-02, -7.2491e-03,  1.0477e-02,  1.3402e-02,\n",
      "          6.7758e-02, -9.4209e-02, -3.7169e-02,  5.2262e-02, -3.1085e-02,\n",
      "         -9.6341e-02,  1.5772e-02,  2.5787e-02,  7.8525e-02,  7.8995e-02,\n",
      "          1.9152e-02,  1.6436e-02,  3.1009e-03,  3.8131e-02,  2.3709e-02,\n",
      "          1.0539e-02, -4.4064e-02,  4.4174e-02, -2.5873e-02,  6.1538e-02,\n",
      "         -4.0543e-02, -8.6414e-02,  3.1972e-02, -8.9065e-04, -2.4444e-02,\n",
      "         -9.1972e-02,  2.3394e-02, -8.3029e-02,  4.4151e-02, -2.4969e-02,\n",
      "          6.2302e-02, -1.3035e-03,  7.5140e-02,  2.4639e-02, -6.4724e-02,\n",
      "         -1.1773e-01,  3.8339e-02, -9.1177e-02,  6.3545e-02,  7.6274e-02,\n",
      "         -8.8024e-02,  9.5456e-03, -4.6972e-02, -8.4174e-02,  3.8882e-02,\n",
      "         -1.1439e-01,  6.2886e-03, -3.4936e-02,  2.3975e-02, -3.3132e-02,\n",
      "         -1.5724e-02, -3.7896e-02, -8.8125e-03,  7.0612e-02,  3.2807e-02,\n",
      "          2.0367e-03, -1.1228e-01,  6.7972e-03,  1.2277e-02,  3.3530e-02,\n",
      "         -1.3620e-02, -2.2549e-02, -2.2523e-02, -2.0319e-02,  5.0430e-02,\n",
      "         -7.4865e-02, -8.2282e-02,  7.6596e-02,  4.9339e-02, -3.7555e-02,\n",
      "          1.4463e-02, -5.7246e-02, -1.7995e-02,  1.0970e-01,  1.1946e-01,\n",
      "          8.0925e-04,  6.1706e-02,  3.2632e-02, -1.3078e-01, -1.4864e-01,\n",
      "         -6.1623e-02,  4.3389e-02,  2.6713e-02,  1.3979e-02, -3.9400e-02,\n",
      "         -2.5271e-02,  3.8774e-03,  3.5866e-02, -6.1542e-02,  3.7666e-02,\n",
      "          2.6757e-02, -3.8266e-02, -3.5479e-02, -2.3923e-02,  8.6798e-02,\n",
      "         -1.8406e-02,  7.7104e-02,  1.3987e-03,  7.0038e-02, -4.7788e-02,\n",
      "         -7.8982e-02,  5.1081e-02, -2.9987e-33, -3.9165e-02, -2.5621e-03,\n",
      "          1.6521e-02,  9.4894e-03, -5.6622e-02,  6.5778e-02, -4.7700e-02,\n",
      "          1.1166e-02, -5.7356e-02, -9.1626e-03, -2.1752e-02, -5.5953e-02,\n",
      "         -1.1142e-02,  9.3279e-02,  1.6677e-02, -1.3672e-02,  4.3439e-02,\n",
      "          1.8724e-03,  7.2995e-03,  5.1633e-02,  4.8061e-02,  1.3534e-01,\n",
      "         -1.7174e-02, -1.2970e-02, -7.5011e-02,  2.6111e-02,  2.6980e-02,\n",
      "          7.8306e-04, -4.8727e-02,  1.1784e-02, -4.5958e-02, -4.8321e-02,\n",
      "         -1.9567e-02,  1.9389e-02,  1.9881e-02,  1.6743e-02,  9.8780e-02,\n",
      "         -2.7409e-02,  2.3481e-02,  3.7023e-03, -6.1451e-02, -1.2123e-03,\n",
      "         -9.5047e-03,  9.2515e-03,  2.3844e-02,  8.6123e-02,  2.2679e-02,\n",
      "          5.4512e-04,  3.4713e-02,  6.2546e-03, -6.9278e-03,  3.9240e-02,\n",
      "          1.1567e-02,  3.2628e-02,  6.2216e-02,  2.7611e-02,  1.8688e-02,\n",
      "          3.5581e-02,  4.1180e-02,  1.5478e-02,  4.2269e-02,  3.8225e-02,\n",
      "          1.0031e-02, -2.8325e-02,  4.4705e-02, -4.1046e-02, -4.5055e-03,\n",
      "         -5.4473e-02,  2.6232e-02,  1.7986e-02, -1.2312e-01, -4.6695e-02,\n",
      "         -1.3591e-02,  6.4671e-02,  3.5735e-03, -1.2223e-02, -1.7938e-02,\n",
      "         -2.5550e-02,  2.3722e-02,  4.0866e-03, -6.5148e-02,  4.4365e-02,\n",
      "          4.6860e-02, -3.2517e-02,  4.0226e-03, -3.9761e-03,  1.1194e-02,\n",
      "         -9.9560e-02,  3.3317e-02,  8.0106e-02,  9.4269e-02, -6.3829e-02,\n",
      "          3.2315e-02, -5.1355e-02, -7.4988e-03,  5.3005e-34, -4.1320e-02,\n",
      "          9.4965e-02, -1.0640e-01,  4.9659e-02, -3.4191e-02, -3.1675e-02,\n",
      "         -1.7156e-02,  1.7010e-03,  5.7976e-02, -1.2178e-03, -1.6854e-02,\n",
      "         -5.1691e-02,  5.5300e-02, -3.4265e-02,  3.0818e-02, -3.1048e-02,\n",
      "          9.2753e-02,  3.7266e-02, -2.3740e-02,  4.4589e-02,  1.4615e-02,\n",
      "          1.1624e-01, -5.0011e-02,  3.8872e-02,  4.2474e-03,  2.5698e-02,\n",
      "          3.2724e-02,  4.2991e-02, -1.3614e-02,  2.5612e-02,  1.0626e-02,\n",
      "         -8.4686e-02, -9.5298e-02,  1.0840e-01, -7.5160e-02, -1.3777e-02,\n",
      "          6.3734e-02, -4.4967e-03, -3.2532e-02,  6.2361e-02,  3.4805e-02,\n",
      "         -3.5492e-02, -2.0022e-02,  3.6661e-02, -2.4884e-02,  1.0182e-02,\n",
      "         -7.0123e-02, -4.3195e-02,  2.9533e-02, -2.9490e-04, -3.4539e-02,\n",
      "          1.4668e-02, -9.8397e-02, -4.7049e-02, -8.8550e-03, -8.8991e-02,\n",
      "          3.5100e-02, -1.2960e-01, -4.9887e-02, -6.1205e-02, -5.9780e-02,\n",
      "          9.4632e-03,  4.9122e-02, -7.7503e-02,  8.0973e-02, -4.7926e-02,\n",
      "          2.3438e-03,  7.5703e-02, -2.4018e-02, -1.5255e-02,  4.8674e-02,\n",
      "         -3.8597e-02, -7.0483e-02, -1.2035e-02, -3.8879e-02, -7.7602e-02,\n",
      "         -1.0724e-02,  1.0419e-02, -2.1375e-02, -9.1739e-02, -1.1134e-02,\n",
      "         -2.9607e-02,  2.4646e-02,  4.6571e-03, -1.6345e-02, -3.9522e-02,\n",
      "          7.7337e-02, -2.8473e-02, -3.6994e-03,  8.2767e-02, -1.1041e-02,\n",
      "          3.1398e-02,  5.3509e-02,  5.7515e-02, -3.1762e-02, -1.5291e-08,\n",
      "         -7.9966e-02, -4.7680e-02, -8.5979e-02,  5.6962e-02, -4.0887e-02,\n",
      "          2.2383e-02, -4.6445e-03, -3.8013e-02, -3.1067e-02, -1.0728e-02,\n",
      "          1.9770e-02,  7.7700e-03, -6.0947e-03, -3.8638e-02,  2.8027e-02,\n",
      "          6.7814e-02, -2.3535e-02,  3.2175e-02,  8.0254e-03, -2.3911e-02,\n",
      "         -1.2200e-03,  3.1460e-02, -5.2492e-02, -8.0682e-03,  3.1478e-03,\n",
      "          5.1150e-02, -4.4410e-02,  6.3601e-02,  3.8508e-02,  3.3043e-02,\n",
      "         -4.1873e-03,  4.9559e-02, -5.6961e-02, -6.4971e-03, -2.4979e-02,\n",
      "         -1.6087e-02,  6.6229e-02, -2.0631e-02,  1.0805e-01,  1.6855e-02,\n",
      "          1.4381e-02, -1.3213e-02, -1.2939e-01,  6.9522e-02, -5.5577e-02,\n",
      "         -6.7541e-02, -5.4582e-03, -6.1359e-03,  3.9084e-02, -6.2878e-02,\n",
      "          3.7406e-02, -1.1657e-02,  1.2915e-02, -5.5250e-02,  5.1608e-02,\n",
      "         -4.3084e-03,  5.8025e-02,  1.8694e-02,  2.2781e-02,  3.2167e-02,\n",
      "          5.3798e-02,  7.0285e-02,  7.4931e-02, -8.4178e-02]])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "from regex import P\n",
    "\n",
    "\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_final_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6273f2935998ed9bb80b3a175896a2488b06412ab449d5916a38f39c7b99953d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
